import streamlit as st
import pandas as pd
import numpy as np
import requests
from datetime import datetime
import time
from streamlit_echarts import st_echarts
import base64
from sklearn.feature_extraction.text import TfidfVectorizer
from collections import Counter
import hazm
from typing import List, Dict
from io import BytesIO

# Configuration
st.set_page_config(
    page_title="google Suggestions",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS with RTL support
st.markdown("""
    <style>
    .reportview-container .main .block-container {
        max-width: 1300px;
    }
    .stButton>button {
        width: 100%;
    }
    .persian-text {
        direction: rtl;
        text-align: right;
        font-family: 'Vazir', 'Iran Sans', sans-serif;
    }
    </style>
""", unsafe_allow_html=True)

class PersianTextAnalyzer:
    """Persian text analysis using Hazm."""
    
    def __init__(self):
        self.normalizer = hazm.Normalizer()
        self.stemmer = hazm.Stemmer()
        self.lemmatizer = hazm.Lemmatizer()
        self.stop_words = hazm.stopwords_list()
    
    def normalize_text(self, text: str) -> str:
        return self.normalizer.normalize(str(text))
    
    def get_tokens(self, text: str) -> List[str]:
        return hazm.word_tokenize(self.normalize_text(text))
    
    def remove_stop_words(self, tokens: List[str]) -> List[str]:
        return [word for word in tokens if word not in self.stop_words]

def get_suggestions(keyword: str, source: str = 'google', language: str = 'english') -> list:
    """Get search suggestions from Google with language support."""
    if source.lower() == 'google':
        url = 'https://suggestqueries.google.com/complete/search'
        params = {
            'client': 'firefox',
            'q': keyword,
            'hl': 'fa' if language == 'persian' else 'en'
        }
    
    try:
        response = requests.get(url, params=params, timeout=5)
        response.raise_for_status()
        suggestions = response.json()[1]
        return suggestions[:5]  # Limit to top 5 suggestions
    except Exception as e:
        st.error(f"Error fetching suggestions: {str(e)}")
        return []

@st.cache_data(ttl=3600)
def get_suggests_tree(keyword: str, source: str = 'google', max_depth: int = 3, language: str = 'english') -> pd.DataFrame:
    """Get a tree of search suggestions with language support."""
    edges = []
    timestamp = datetime.now()
    
    with st.spinner('Fetching suggestions...'):
        progress_bar = st.progress(0)
        
        # Level 1
        level1_suggests = get_suggestions(keyword, source, language)
        progress_bar.progress(33)
        
        for rank1, edge1 in enumerate(level1_suggests):
            edges.append({
                'root': keyword,
                'edge': edge1,
                'rank': rank1,
                'depth': 1,
                'search_engine': source,
                'datetime': timestamp,
                'level1': edge1,
                'level2': None,
                'level3': None
            })
            
            if max_depth >= 2:
                time.sleep(0.2)
                level2_suggests = get_suggestions(edge1, source, language)
                progress_bar.progress(66)
                
                for rank2, edge2 in enumerate(level2_suggests):
                    edges.append({
                        'root': keyword,
                        'edge': edge2,
                        'rank': rank2,
                        'depth': 2,
                        'search_engine': source,
                        'datetime': timestamp,
                        'level1': edge1,
                        'level2': edge2,
                        'level3': None
                    })
                    
                    if max_depth >= 3:
                        time.sleep(0.2)
                        level3_suggests = get_suggestions(edge2, source, language)
                        
                        for rank3, edge3 in enumerate(level3_suggests):
                            edges.append({
                                'root': keyword,
                                'edge': edge3,
                                'rank': rank3,
                                'depth': 3,
                                'search_engine': source,
                                'datetime': timestamp,
                                'level1': edge1,
                                'level2': edge2,
                                'level3': edge3
                            })
        
        progress_bar.progress(100)
        time.sleep(0.1)
        progress_bar.empty()
    
    return pd.DataFrame(edges)

def analyze_suggestions(df: pd.DataFrame, language: str = 'english') -> pd.DataFrame:
    """Analyze suggestions with language support."""
    enhanced_df = df.copy()
    
    if language == 'persian':
        analyzer = PersianTextAnalyzer()
        tokenize_func = analyzer.get_tokens
        normalize_func = analyzer.normalize_text
        stop_words = analyzer.stop_words
    else:
        from nltk.tokenize import word_tokenize
        from nltk.corpus import stopwords
        import nltk
        try:
            nltk.data.find('tokenizers/punkt')
            nltk.data.find('corpora/stopwords')
        except LookupError:
            nltk.download('punkt')
            nltk.download('stopwords')
        
        tokenize_func = word_tokenize
        normalize_func = str.lower
        stop_words = set(stopwords.words('english'))
    
    # Text analysis
    suggestions_text = enhanced_df['edge'].fillna('').apply(normalize_func).tolist()
    
    # Word count
    enhanced_df['word_count'] = enhanced_df['edge'].fillna('').apply(
        lambda x: len(tokenize_func(normalize_func(x)))
    )
    
    # Character count
    enhanced_df['char_count'] = enhanced_df['edge'].fillna('').apply(len)
    
    # Word repetitions
    def count_word_repetitions(text):
        tokens = tokenize_func(normalize_func(text))
        return sum(count - 1 for count in Counter(tokens).values())
    
    enhanced_df['word_repetitions'] = enhanced_df['edge'].apply(count_word_repetitions)
    
    # TF-IDF
    tfidf = TfidfVectorizer(tokenizer=tokenize_func, preprocessor=normalize_func)
    tfidf_matrix = tfidf.fit_transform(suggestions_text)
    enhanced_df['avg_tfidf'] = np.array(tfidf_matrix.mean(axis=1)).flatten()
    
    # Keyword density
    def calculate_keyword_density(text, keyword):
        text_norm = normalize_func(text)
        keyword_norm = normalize_func(keyword)
        tokens = tokenize_func(text_norm)
        keyword_tokens = tokenize_func(keyword_norm)
        keyword_count = sum(1 for token in tokens if token in keyword_tokens)
        return (keyword_count / len(tokens) * 100) if tokens else 0
    
    enhanced_df['keyword_density'] = enhanced_df['edge'].apply(
        lambda x: calculate_keyword_density(x, enhanced_df['root'].iloc[0])
    )
    
    # Stop words ratio
    def calculate_stop_words_ratio(text):
        tokens = tokenize_func(normalize_func(text))
        stop_count = sum(1 for token in tokens if token in stop_words)
        return stop_count / len(tokens) if tokens else 0
    
    enhanced_df['stop_words_ratio'] = enhanced_df['edge'].apply(calculate_stop_words_ratio)
    
    # Unique words ratio
    def unique_words_ratio(text):
        tokens = tokenize_func(normalize_func(text))
        return len(set(tokens)) / len(tokens) if tokens else 0
    
    enhanced_df['unique_words_ratio'] = enhanced_df['edge'].apply(unique_words_ratio)
    
    # Complexity score
    enhanced_df['complexity_score'] = (
        enhanced_df['word_count'] * 0.25 +
        enhanced_df['unique_words_ratio'] * 0.25 +
        enhanced_df['avg_tfidf'] * 0.25 +
        (1 - enhanced_df['stop_words_ratio']) * 0.25
    ).round(2)
    
    # Round floating point columns
    float_columns = ['avg_tfidf', 'keyword_density', 'stop_words_ratio', 'unique_words_ratio']
    enhanced_df[float_columns] = enhanced_df[float_columns].round(3)
    
    return enhanced_df

def create_download_links(df: pd.DataFrame, language: str = 'english'):
    """Create download links with language support."""
    # Excel download
    output = BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df.to_excel(writer, index=False)
    excel_data = output.getvalue()
    b64_excel = base64.b64encode(excel_data).decode()
    
    # CSV download
    csv_data = df.to_csv(index=False)
    b64_csv = base64.b64encode(csv_data.encode()).decode()
    
    col1, col2 = st.columns(2)
    with col1:
        st.markdown(
            f'<a href="data:application/vnd.openxmlformats-officedocument.spreadsheetml.sheet;base64,{b64_excel}" download="suggestions.xlsx">{"Ø¯Ø§Ù†Ù„ÙˆØ¯ Excel" if language == "persian" else "Download Excel"} ğŸ“¥</a>',
            unsafe_allow_html=True
        )
    
    with col2:
        st.markdown(
            f'<a href="data:text/csv;base64,{b64_csv}" download="suggestions.csv">{"Ø¯Ø§Ù†Ù„ÙˆØ¯ CSV" if language == "persian" else "Download CSV"} ğŸ“¥</a>',
            unsafe_allow_html=True
        )

def main():
    # Header
    col1, _, col3 = st.columns(3)
    with col1:
        st.title("google Suggestions ğŸ”")
    with col3:
        st.write("")
        st.write("")
        st.markdown(
            '###### Made with :heart: by [@dadashzadeh](https://t.me/Dadashzadeh)'
        )
    
    # Language selection
    language = st.selectbox(
        'Select Language / Ø§Ù†ØªØ®Ø§Ø¨ Ø²Ø¨Ø§Ù†',
        ('english', 'persian')
    )
    
    # Info sections
    with st.expander("â„¹ï¸ About this app", expanded=True):
        if language == 'persian':
            st.write("""
            - Ø¨Ù‡ Ø´Ù…Ø§ Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡ Ø±Ø§ Ú©Ø´Ù Ùˆ ØªØ¬Ø³Ù… Ú©Ù†ÛŒØ¯!
            - Ø§Ø² Ù…ÙˆØªÙˆØ± Ø¬Ø³ØªØ¬ÙˆÛŒ Ú¯ÙˆÚ¯Ù„ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
            - ØªØ§ Û³ Ø³Ø·Ø­ Ø¹Ù…Ù‚ Ø±Ø§ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
            - Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ ØªØ­Ù‚ÛŒÙ‚ Ù…Ø­ØªÙˆØ§ Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ SEO
            """)
        else:
            st.write("""
            - helps you discover and visualize keyword suggestions!
            - Supports Google search engine
            - Supports up to 3 levels of suggestions
            - Perfect for content research and SEO optimization
            """)
    
    # User Input
    keyword = st.text_input(
        'Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯' if language == 'persian' else 'Enter your keyword'
    )
    
    col1, col2 = st.columns(2)
    with col1:
        search_engine = 'google'
        st.info("Google Search" if language == 'english' else "Ø¬Ø³ØªØ¬ÙˆÛŒ Ú¯ÙˆÚ¯Ù„")
    
    with col2:
        max_depth = st.number_input(
            'Ø¹Ù…Ù‚ Ø¯Ø±Ø®Øª (Û±-Û³ Ø³Ø·Ø­)' if language == 'persian' else 'Tree Depth (1-3 levels)',
            min_value=1,
            max_value=3,
            value=2
        )
    
    st.info(
        "Ø³Ø·ÙˆØ­ Ø¨Ø§Ù„Ø§ØªØ± Ø²Ù…Ø§Ù† Ø¨ÛŒØ´ØªØ±ÛŒ Ù†ÛŒØ§Ø² Ø¯Ø§Ø±Ù†Ø¯" if language == 'persian' 
        else "Higher depth levels will take longer to process"
    )
    
    fetch_button = st.button(
        'ğŸ”¥ Ø¯Ø±ÛŒØ§ÙØª Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª' if language == 'persian' else 'Fetch Suggestions! ğŸ”¥'
    )
    
    if fetch_button and keyword:
        try:
            # Fetch suggestions
            suggestions_df = get_suggests_tree(keyword, search_engine, max_depth, language)
            
            if not suggestions_df.empty:
                # Analyze suggestions
                enhanced_df = analyze_suggestions(suggestions_df, language)
                
                # Build tree data
                tree_data = {
                    "name": keyword,
                    "children": [
                        {
                            "name": group['level1'],
                            "children": [
                                {
                                    "name": row['level2'],
                                    "children": [
                                        {"name": row2['level3']}
                                        for _, row2 in enhanced_df[
                                            (enhanced_df['level1'] == group['level1']) & 
                                            (enhanced_df['level2'] == row['level2']) & 
                                            (enhanced_df['depth'] == 3)
                                        ].iterrows()
                                    ]
                                }
                                for _, row in enhanced_df[
                                    (enhanced_df['level1'] == group['level1']) & 
                                    (enhanced_df['depth'] == 2)
                                ].iterrows()
                            ]
                        }
                        for _, group in enhanced_df[enhanced_df['depth'] == 1].iterrows()
                    ]
                }
                
                # Tree visualization
                tree_config = {
                    "tooltip": {"trigger": "item", "triggerOn": "mousemove"},
                    "series": [{
                        "type": "tree",
                        "data": [tree_data],
                        "top": "1%",
                        "left": "7%",
                        "bottom": "1%",
                        "right": "20%",
                        "symbolSize": 7,
                        "initialTreeDepth": max_depth,
                        "label": {
                            "position": "left",
                            "verticalAlign": "middle",
                            "align": "right",
                            "fontSize": 12,
                        },
                        "leaves": {
                            "label": {
                                "position": "right",
                                "verticalAlign": "middle",
                                "align": "left",
                            }
                        },
                        "expandAndCollapse": True,
                        "animationDuration": 550,
                        "animationDurationUpdate": 750,
                        "layout": "orthogonal",
                        "orient": "LR"
                    }]
                }
                
                st.success(
                    'âœ… Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯Ù†Ø¯!' if language == 'persian'
                    else 'âœ… Suggestions retrieved successfully!'
                )
                # Display results
                st.markdown(
                    '### ğŸŒ³ Ù†Ù…Ø§ÛŒ Ø¯Ø±Ø®ØªÛŒ ØªØ¹Ø§Ù…Ù„ÛŒ' if language == 'persian'
                    else '### ğŸŒ³ Interactive Tree View'
                )
                st.markdown(
                    '*Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ ØªØµÙˆÛŒØ± Ú©Ù„ÛŒÚ© Ø±Ø§Ø³Øª Ú©Ù†ÛŒØ¯* ğŸ“¸' if language == 'persian'
                    else '*Right-click to save as image* ğŸ“¸'
                )
                st_echarts(options=tree_config, height="800px")
                
                # Display enhanced DataFrame
                st.markdown(
                    '### ğŸ“Š Ù†Ù…Ø§ÛŒ Ø¬Ø¯ÙˆÙ„ ØªØ­Ù„ÛŒÙ„ÛŒ' if language == 'persian'
                    else '### ğŸ“Š Analysis Table'
                )
                
                # Define column configuration based on language
                if language == 'persian':
                    column_config = {
                        'word_count': st.column_config.NumberColumn(
                            'ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª',
                            help='ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª Ø¯Ø± Ù‡Ø± Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯'
                        ),
                        'char_count': st.column_config.NumberColumn(
                            'ØªØ¹Ø¯Ø§Ø¯ Ø­Ø±ÙˆÙ',
                            help='ØªØ¹Ø¯Ø§Ø¯ Ø­Ø±ÙˆÙ Ø¯Ø± Ù‡Ø± Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯'
                        ),
                        'word_repetitions': st.column_config.NumberColumn(
                            'ØªÚ©Ø±Ø§Ø± Ú©Ù„Ù…Ø§Øª',
                            help='ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª ØªÚ©Ø±Ø§Ø±ÛŒ'
                        ),
                        'avg_tfidf': st.column_config.NumberColumn(
                            'Ù†Ù…Ø±Ù‡ TF-IDF',
                            help='Ù…Ø¹ÛŒØ§Ø± Ø§Ù‡Ù…ÛŒØª Ú©Ù„Ù…Ø§Øª',
                            format="%.3f"
                        ),
                        'keyword_density': st.column_config.NumberColumn(
                            'ØªØ±Ø§Ú©Ù… Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡',
                            help='Ø¯Ø±ØµØ¯ ØªÚ©Ø±Ø§Ø± Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡ Ø§ØµÙ„ÛŒ',
                            format="%.2f%%"
                        ),
                        'stop_words_ratio': st.column_config.NumberColumn(
                            'Ù†Ø³Ø¨Øª Ú©Ù„Ù…Ø§Øª Ø¹Ù…ÙˆÙ…ÛŒ',
                            help='Ù†Ø³Ø¨Øª Ú©Ù„Ù…Ø§Øª Ù¾Ø±Ú©Ø§Ø±Ø¨Ø±Ø¯ Ø¨Ù‡ Ú©Ù„',
                            format="%.2f"
                        ),
                        'unique_words_ratio': st.column_config.NumberColumn(
                            'Ù†Ø³Ø¨Øª Ú©Ù„Ù…Ø§Øª ÛŒÚ©ØªØ§',
                            help='Ù†Ø³Ø¨Øª Ú©Ù„Ù…Ø§Øª ØºÛŒØ±ØªÚ©Ø±Ø§Ø±ÛŒ Ø¨Ù‡ Ú©Ù„',
                            format="%.2f"
                        ),
                        'complexity_score': st.column_config.NumberColumn(
                            'Ù†Ù…Ø±Ù‡ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ',
                            help='Ù…Ø¹ÛŒØ§Ø± Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ Ù…ØªÙ† (0-1)',
                            format="%.2f"
                        )
                    }
                else:
                    column_config = {
                        'word_count': st.column_config.NumberColumn(
                            'Word Count',
                            help='Number of words in suggestion'
                        ),
                        'char_count': st.column_config.NumberColumn(
                            'Char Count',
                            help='Number of characters'
                        ),
                        'word_repetitions': st.column_config.NumberColumn(
                            'Word Repetitions',
                            help='Number of repeated words'
                        ),
                        'avg_tfidf': st.column_config.NumberColumn(
                            'TF-IDF Score',
                            help='Term Frequency-Inverse Document Frequency score',
                            format="%.3f"
                        ),
                        'keyword_density': st.column_config.NumberColumn(
                            'Keyword Density',
                            help='Percentage of keyword occurrence',
                            format="%.2f%%"
                        ),
                        'stop_words_ratio': st.column_config.NumberColumn(
                            'Stop Words Ratio',
                            help='Ratio of common words',
                            format="%.2f"
                        ),
                        'unique_words_ratio': st.column_config.NumberColumn(
                            'Unique Words Ratio',
                            help='Ratio of unique words to total words',
                            format="%.2f"
                        ),
                        'complexity_score': st.column_config.NumberColumn(
                            'Complexity Score',
                            help='Custom complexity metric (0-1)',
                            format="%.2f"
                        )
                    }
                
                # Create tabs for different views
                tab1, tab2 = st.tabs(
                    ["Ù†Ù…Ø§ÛŒ Ø³Ø§Ø¯Ù‡", "ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒØ´Ø±ÙØªÙ‡"] if language == 'persian'
                    else ["Basic View", "Advanced Analysis"]
                )
                
                with tab1:
                    basic_columns = ['root', 'edge', 'depth', 'word_count', 
                                   'keyword_density', 'complexity_score']
                    st.dataframe(
                        enhanced_df[basic_columns],
                        column_config=column_config,
                        hide_index=True
                    )
                
                with tab2:
                    st.dataframe(
                        enhanced_df,
                        column_config=column_config,
                        hide_index=True
                    )
                
                # Add download section
                st.markdown(
                    '### ğŸ“¥ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù†ØªØ§ÛŒØ¬' if language == 'persian'
                    else '### ğŸ“¥ Download Results'
                )
                create_download_links(enhanced_df, language)
                
                # Add metrics summary
                st.markdown(
                    '### ğŸ“Š Ø®Ù„Ø§ØµÙ‡ ØªØ­Ù„ÛŒÙ„' if language == 'persian'
                    else '### ğŸ“Š Analysis Summary'
                )
                
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric(
                        "Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ú©Ù„Ù…Ø§Øª" if language == 'persian' else "Average Words",
                        f"{enhanced_df['word_count'].mean():.1f}",
                        f"{enhanced_df['word_count'].std():.1f} Ïƒ"
                    )
                
                with col2:
                    st.metric(
                        "Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ØªØ±Ø§Ú©Ù… Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡" if language == 'persian' else "Avg Keyword Density",
                        f"{enhanced_df['keyword_density'].mean():.1f}%",
                        f"{enhanced_df['keyword_density'].std():.1f}% Ïƒ"
                    )
                
                with col3:
                    st.metric(
                        "Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ" if language == 'persian' else "Avg Complexity",
                        f"{enhanced_df['complexity_score'].mean():.2f}",
                        f"{enhanced_df['complexity_score'].std():.2f} Ïƒ"
                    )
                
                with col4:
                    st.metric(
                        "Ù†Ø³Ø¨Øª Ú©Ù„Ù…Ø§Øª ÛŒÚ©ØªØ§" if language == 'persian' else "Unique Words Ratio",
                        f"{enhanced_df['unique_words_ratio'].mean():.2f}",
                        f"{enhanced_df['unique_words_ratio'].std():.2f} Ïƒ"
                    )
                
            else:
                st.warning(
                    'Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡ Ø¯ÛŒÚ¯Ø±ÛŒ Ø±Ø§ Ø§Ù…ØªØ­Ø§Ù† Ú©Ù†ÛŒØ¯.' if language == 'persian'
                    else 'No suggestions found. Try a different keyword.'
                )
                
        except Exception as e:
            st.error(
                f'Ø®Ø·Ø§ÛŒÛŒ Ø±Ø® Ø¯Ø§Ø¯: {str(e)}' if language == 'persian'
                else f'An error occurred: {str(e)}'
            )
            
    elif fetch_button:
        st.warning(
            'Ù„Ø·ÙØ§Ù‹ ÛŒÚ© Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯.' if language == 'persian'
            else 'Please enter a keyword first.'
        )

if __name__ == "__main__":
    main()
